barrier.h: * JOIN-LEAVE SEMANTICS: Threads can join or leave the barrier at any point in time.
barrier.h: * Threads in the barrier call sync and synchronize with all other threads
barrier.h: * participating in the barrier. Threads can leave a barrier at any point in time
barrier.h: * (e.g. when other threads have started the sync).
barrier.h: * PARALLELISM CONTROL: The barrier limits the number of threads that run at the same time.
barrier.h:        uint32_t parallelThreads;
barrier.h:        struct ThreadSyncInfo {
barrier.h:        ThreadSyncInfo threadList[MAX_THREADS];
barrier.h:        uint32_t curThreadIdx;
barrier.h:        uint32_t runningThreads; //threads in RUNNING state
barrier.h:        uint32_t leftThreads; //threads in LEFT state
barrier.h:        //Threads in OFFLINE state are not on the runlist, so runListSize - runningThreads - leftThreads == waitingThreads
barrier.h:         * hierarchy (which use yield, not futex?). The lock-free version was actually a bit slower, as we're already serializing on curThreadIdx and
barrier.h:        Barrier(uint32_t _parallelThreads, Callee* _sched) : parallelThreads(_parallelThreads), rnd(0xBA77137), sched(_sched) {
barrier.h:            for (uint32_t t = 0; t < MAX_THREADS; t++) {
barrier.h:                threadList[t].state = OFFLINE;
barrier.h:                threadList[t].futexWord = 0;
barrier.h:            runList = gm_calloc<uint32_t>(MAX_THREADS);
barrier.h:            curThreadIdx = 0;
barrier.h:            runningThreads = 0;
barrier.h:            leftThreads = 0;
barrier.h:            DEBUG_BARRIER("[%d] Joining, runningThreads %d, prevState %d", tid, runningThreads, threadList[tid].state);
barrier.h:            assert(threadList[tid].state == LEFT || threadList[tid].state == OFFLINE);
barrier.h:            if (threadList[tid].state == OFFLINE) {
barrier.h:                leftThreads--;
barrier.h:                uint32_t lastIdx = threadList[tid].lastIdx;
barrier.h:                if (curThreadIdx > lastIdx) { //curThreadIdx points to the FIRST thread that tryWakeNext checks
barrier.h:                    curThreadIdx--;
barrier.h:                    //Swap our runlist tid with the last thread's
barrier.h:                    uint32_t otherTid = runList[curThreadIdx];
barrier.h:                    runList[curThreadIdx] = tid;
barrier.h:                    threadList[otherTid].lastIdx = lastIdx;
barrier.h:                    threadList[tid].lastIdx = curThreadIdx;
barrier.h:            threadList[tid].state = WAITING;
barrier.h:            threadList[tid].futexWord = 1;
barrier.h:            if (threadList[tid].state == WAITING) {
barrier.h:                    int futex_res = syscall(SYS_futex, &threadList[tid].futexWord, FUTEX_WAIT, 1 /*a racing thread waking us up will change value to 0, and we won't block*/, nullptr, nullptr, 0);
barrier.h:                    if (futex_res == 0 || threadList[tid].futexWord != 1) break;
barrier.h:                //The thread that wakes us up changes this
barrier.h:                assert(threadList[tid].state == RUNNING);
barrier.h:            DEBUG_BARRIER("[%d] Leaving, runningThreads %d", tid, runningThreads);
barrier.h:            if (threadList[tid].state == RUNNING) {
barrier.h:                threadList[tid].state = LEFT;
barrier.h:                leftThreads++;
barrier.h:                runningThreads--;
barrier.h:                assert_msg(threadList[tid].state == WAITING, "leave, tid %d, incorrect state %d", tid, threadList[tid].state);
barrier.h:                threadList[tid].state = LEFT;
barrier.h:                leftThreads++;
barrier.h:            assert_msg(threadList[tid].state == RUNNING, "[%d] sync: state was supposed to be %d, it is %d", tid, RUNNING, threadList[tid].state);
barrier.h:            threadList[tid].futexWord = 1;
barrier.h:            threadList[tid].state = WAITING;
barrier.h:            runningThreads--;
barrier.h:            if (threadList[tid].state == WAITING) {
barrier.h:                    int futex_res = syscall(SYS_futex, &threadList[tid].futexWord, FUTEX_WAIT, 1 /*a racing thread waking us up will change value to 0, and we won't block*/, nullptr, nullptr, 0);
barrier.h:                    if (futex_res == 0 || threadList[tid].futexWord != 1) break;
barrier.h:                //The thread that wakes us up changes this
barrier.h:                assert(threadList[tid].state == RUNNING);
barrier.h:            if (curThreadIdx == runListSize && runningThreads == 0) {
barrier.h:                if (leftThreads == runListSize) {
barrier.h:                    DEBUG_BARRIER("[%d] All threads left barrier, not ending current phase", tid);
barrier.h:                curThreadIdx = 0; //rewind list
barrier.h:                    /* Pass over the whole array, OFFLINE the threads that LEFT. If they are on a syscall, they will rejoin;
barrier.h:                     * If they left for good, we avoid long-term traversal overheads on apps with a varying number of threads.
barrier.h:                        if (threadList[wtid].state == LEFT) {
barrier.h:                            threadList[wtid].state = OFFLINE;
barrier.h:                            threadList[stid].lastIdx = idx;
barrier.h:                    assert(runListSize - newSize == leftThreads);
barrier.h:                    leftThreads = 0;
barrier.h:                //NOTE: If this is a performance hog, the algorithm can be rewritten to be top-down and threads can be woken up as soon as they are reordered. So far, I've seen this has negligible overheads though.
barrier.h:                if (parallelThreads < runListSize) {
barrier.h:                    //Randomly shuffle thread list to avoid systemic biases and reduce contention on cache hierarchy (Fisher-Yates shuffle)
barrier.h:                        threadList[itid].lastIdx = j;
barrier.h:                        threadList[jtid].lastIdx = i;
barrier.h:            while (runningThreads < parallelThreads && curThreadIdx < runListSize) {
barrier.h:                //Wake next thread
barrier.h:                uint32_t idx = curThreadIdx++;
barrier.h:                if (threadList[wtid].state == WAITING) {
barrier.h:                    DEBUG_BARRIER("[%d] Waking %d runningThreads %d", tid, wtid, runningThreads);
barrier.h:                    threadList[wtid].state = RUNNING; //must be set before writing to futexWord to avoid wakeup race
barrier.h:                    threadList[wtid].lastIdx = idx;
barrier.h:                    bool succ = __sync_bool_compare_and_swap(&threadList[wtid].futexWord, 1, 0);
barrier.h:                    syscall(SYS_futex, &threadList[wtid].futexWord, FUTEX_WAKE, 1, nullptr, nullptr, 0);
barrier.h:                    runningThreads++;
barrier.h:                    DEBUG_BARRIER("[%d] Skipping %d state %d", tid, wtid, threadList[wtid].state);
barrier.h:            checkRunList(tid); //wake up threads on this phase, may reach EOP
barrier.h:            checkRunList(tid); //if we started a new phase, wake up threads
constants.h:// PIN 2.9 (rev39599) can't do more than 2048 threads...
constants.h:#define MAX_THREADS (2048)
contention_sim.cpp:void ContentionSim::SimThreadTrampoline(void* arg) {
contention_sim.cpp:    uint32_t thid = __sync_fetch_and_add(&csim->threadTicket, 1);
contention_sim.cpp:    csim->simThreadLoop(thid);
contention_sim.cpp:ContentionSim::ContentionSim(uint32_t _numDomains, uint32_t _numSimThreads) {
contention_sim.cpp:    numSimThreads = _numSimThreads;
contention_sim.cpp:    threadsDone = 0;
contention_sim.cpp:    simThreads = gm_calloc<SimThreadData>(numSimThreads);
contention_sim.cpp:    if ((numDomains % numSimThreads) != 0) panic("numDomains(%d) must be a multiple of numSimThreads(%d) for now", numDomains, numSimThreads);
contention_sim.cpp:    for (uint32_t i = 0; i < numSimThreads; i++) {
contention_sim.cpp:        futex_init(&simThreads[i].wakeLock);
contention_sim.cpp:        futex_lock(&simThreads[i].wakeLock); //starts locked, so first actual call to lock blocks
contention_sim.cpp:        simThreads[i].firstDomain = i*numDomains/numSimThreads;
contention_sim.cpp:        simThreads[i].supDomain = (i+1)*numDomains/numSimThreads;
contention_sim.cpp:    //Launch domain simulation threads
contention_sim.cpp:    threadTicket = 0;
contention_sim.cpp:    for (uint32_t i = 0; i < numSimThreads; i++) {
contention_sim.cpp:        PIN_SpawnInternalThread(SimThreadTrampoline, this, 1024*1024, nullptr);
contention_sim.cpp:    lastCrossing = gm_calloc<CrossingEventInfo>(numDomains*numDomains*MAX_THREADS); //TODO: refine... this allocs too much
contention_sim.cpp:    //Wake up sim threads
contention_sim.cpp:    for (uint32_t i = 0; i < numSimThreads; i++) {
contention_sim.cpp:        futex_unlock(&simThreads[i].wakeLock);
contention_sim.cpp:void ContentionSim::simThreadLoop(uint32_t thid) {
contention_sim.cpp:    info("Started contention simulation thread %d", thid);
contention_sim.cpp:    //CPU_SET(0, cpuset); CPU_SET(1, cpuset); CPU_SET(2, cpuset); CPU_SET(3, cpuset); //don't use hyperthreads; confuses the scheduler to no end, does worse
contention_sim.cpp:    CPU_SET(thid % nprocs, &cpuset); CPU_SET((thid % nprocs) + (nprocs/2), &cpuset); //pin to a single core but can use both its hyperthreads, works best (~20% gain)
contention_sim.cpp:    int r = sched_setaffinity(0 /*calling thread, equiv to syscall(SYS_gettid)*/, sizeof(cpuset), &cpuset);
contention_sim.cpp:        futex_lock_nospin(&simThreads[thid].wakeLock);
contention_sim.cpp:        simulatePhaseThread(thid);
contention_sim.cpp:        uint32_t val = __sync_add_and_fetch(&threadsDone, 1);
contention_sim.cpp:        if (val == numSimThreads) {
contention_sim.cpp:            threadsDone = 0;
contention_sim.cpp:    info("Finished contention simulation thread %d", thid);
contention_sim.cpp:void ContentionSim::simulatePhaseThread(uint32_t thid) {
contention_sim.cpp:    uint32_t thDomains = simThreads[thid].supDomain - simThreads[thid].firstDomain;
contention_sim.cpp:        DomainData& domain = domains[simThreads[thid].firstDomain];
contention_sim.cpp:            simThreads[thid].logVec.push_back(std::make_pair(cycle, te));
contention_sim.cpp:            for (std::pair<uint64_t, TimingEvent*> p : simThreads[thid].logVec) {
contention_sim.cpp:        simThreads[thid].logVec.clear();
contention_sim.cpp:        //info("XXX %d / %d %d %d", thid, thDomains, simThreads[thid].supDomain, simThreads[thid].firstDomain);
contention_sim.cpp:        for (uint32_t i = simThreads[thid].firstDomain; i < simThreads[thid].supDomain; i++) {
contention_sim.h:            //lock_t domainLock; //used by simulation thread
contention_sim.h:        struct SimThreadData {
contention_sim.h:            lock_t wakeLock; //used to sleep/wake up simulation thread
contention_sim.h:        SimThreadData* simThreads;
contention_sim.h:        uint32_t numSimThreads;
contention_sim.h:        volatile uint32_t threadsDone;
contention_sim.h:        volatile uint32_t threadTicket; //used only at init
contention_sim.h:        ContentionSim(uint32_t _numDomains, uint32_t _numSimThreads);
contention_sim.h:        void simThreadLoop(uint32_t thid);
contention_sim.h:        void simulatePhaseThread(uint32_t thid);
contention_sim.h:        static void SimThreadTrampoline(void* arg);
core.h:    void (*loadPtr)(THREADID, ADDRINT);
core.h:    void (*storePtr)(THREADID, ADDRINT);
core.h:    void (*bblPtr)(THREADID, ADDRINT, BblInfo*);
core.h:    void (*branchPtr)(THREADID, ADDRINT, BOOL, ADDRINT, ADDRINT);
core.h:    void (*predLoadPtr)(THREADID, ADDRINT, BOOL);
core.h:    void (*predStorePtr)(THREADID, ADDRINT, BOOL);
core_recorder.cpp:        //NOTE: Only the first TimingCoreEvent after a thread join needs to be in a domain, hence the default parameter. Because these are inherently sequential and have a fixed delay, subsequent events can inherit the parent's domain, reducing domain xings and improving slack and performance
cpuenum.h:// that can run a thread from the specified pid
galloc.cpp:    volatile void* base_regp; //common data structure, accessible with glob_ptr; threads poll on gm_isready to determine when everything has been initialized
g_heap/dlmalloc.h.c://Do not use locks, even though this is multithreaded. Locking is done externally to dlmalloc,
g_heap/dlmalloc.h.c://because dlmalloc locks are hard to make work with Pin (multiprocess, and pthread_self() does not work)
g_heap/dlmalloc.h.c:  Thread-safety: NOT thread-safe unless USE_LOCKS defined
g_heap/dlmalloc.h.c:       etc is surrounded with either a pthread mutex or a win32
g_heap/dlmalloc.h.c:  You can similarly create thread-local allocators by storing
g_heap/dlmalloc.h.c:  mspaces as thread-locals. For example:
g_heap/dlmalloc.h.c:    static __thread mspace tlms = 0;
g_heap/dlmalloc.h.c:  pthread or WIN32 mutex lock/unlock. (If set true, this can be
g_heap/dlmalloc.h.c:#include <pthread.h>
g_heap/dlmalloc.h.c:#include <thread.h>
g_heap/dlmalloc.h.c:  threads.  This does not protect against direct calls to MORECORE
g_heap/dlmalloc.h.c:  by other threads not using this lock, so there is still code to
g_heap/dlmalloc.h.c:/* Custom pthread-style spin locks on x86 and x64 for gcc */
g_heap/dlmalloc.h.c:/* dsm: By default, dlmalloc uses pthread_self to identify each thread, and to implement recursive locking.
g_heap/dlmalloc.h.c: * This is insufficient across multiple processes, because we can have pthread_self return the same value across them
g_heap/dlmalloc.h.c: * but that could make things much slower, since it's a syscall; instead, lets have the thread id be the concatenation
g_heap/dlmalloc.h.c: * of pthread_self and the pid, which is cached by glibc.
g_heap/dlmalloc.h.c: * IMPORTANT: This doesn't work either, because pthread_self() does not work inside Pin. Rather than resorting to Pin-specific
g_heap/dlmalloc.h.c:struct threadid_t {
g_heap/dlmalloc.h.c:    pthread_t tid;
g_heap/dlmalloc.h.c:    bool is_current() {return (getpid() == pid) && (pthread_self() == tid);}
g_heap/dlmalloc.h.c:struct pthread_mlock_t {
g_heap/dlmalloc.h.c:  threadid_t threadid;
g_heap/dlmalloc.h.c:#define MLOCK_T               struct pthread_mlock_t
g_heap/dlmalloc.h.c:#define CURRENT_THREAD        {getpid(), pthread_self()}
g_heap/dlmalloc.h.c:#define NULL_THREAD           {0, 0}
g_heap/dlmalloc.h.c:#define INITIAL_LOCK(sl)      ((sl)->threadid = NULL_THREAD, (sl)->l = (sl)->c = 0, 0)
g_heap/dlmalloc.h.c:#define ACQUIRE_LOCK(sl)      pthread_acquire_lock(sl)
g_heap/dlmalloc.h.c:#define RELEASE_LOCK(sl)      pthread_release_lock(sl)
g_heap/dlmalloc.h.c:#define TRY_LOCK(sl)          pthread_try_lock(sl)
g_heap/dlmalloc.h.c:static MLOCK_T malloc_global_mutex = { 0, 0, NULL_THREAD};
g_heap/dlmalloc.h.c:static FORCEINLINE int pthread_acquire_lock (MLOCK_T *sl) {
g_heap/dlmalloc.h.c:      if (sl->threadid.is_current()) {
g_heap/dlmalloc.h.c:        assert(sl->threadid.is_empty());
g_heap/dlmalloc.h.c:        sl->threadid = CURRENT_THREAD;
g_heap/dlmalloc.h.c:static FORCEINLINE void pthread_release_lock (MLOCK_T *sl) {
g_heap/dlmalloc.h.c:  assert_msg(sl->threadid.is_current(), "%d|%ld != %d|%ld", sl->threadid.pid, sl->threadid.tid, getpid(), pthread_self());
g_heap/dlmalloc.h.c:    sl->threadid = NULL_THREAD;
g_heap/dlmalloc.h.c:static FORCEINLINE int pthread_try_lock (MLOCK_T *sl) {
g_heap/dlmalloc.h.c:    if (sl->threadid.is_current()) {
g_heap/dlmalloc.h.c:      assert(sl->threadid.is_empty());
g_heap/dlmalloc.h.c:      sl->threadid = CURRENT_THREAD;
g_heap/dlmalloc.h.c:  long threadid;
g_heap/dlmalloc.h.c:#define CURRENT_THREAD        GetCurrentThreadId()
g_heap/dlmalloc.h.c:#define INITIAL_LOCK(sl)      ((sl)->threadid = 0, (sl)->l = (sl)->c = 0, 0)
g_heap/dlmalloc.h.c:      if (sl->threadid == CURRENT_THREAD) {
g_heap/dlmalloc.h.c:        assert(!sl->threadid);
g_heap/dlmalloc.h.c:        sl->threadid = CURRENT_THREAD;
g_heap/dlmalloc.h.c:  assert(sl->threadid == CURRENT_THREAD);
g_heap/dlmalloc.h.c:    sl->threadid = 0;
g_heap/dlmalloc.h.c:    if (sl->threadid == CURRENT_THREAD) {
g_heap/dlmalloc.h.c:      assert(!sl->threadid);
g_heap/dlmalloc.h.c:      sl->threadid = CURRENT_THREAD;
g_heap/dlmalloc.h.c:/* pthreads-based locks */
g_heap/dlmalloc.h.c:#define MLOCK_T               pthread_mutex_t
g_heap/dlmalloc.h.c:#define CURRENT_THREAD        pthread_self()
g_heap/dlmalloc.h.c:#define INITIAL_LOCK(sl)      pthread_init_lock(sl)
g_heap/dlmalloc.h.c:#define ACQUIRE_LOCK(sl)      pthread_mutex_lock(sl)
g_heap/dlmalloc.h.c:#define RELEASE_LOCK(sl)      pthread_mutex_unlock(sl)
g_heap/dlmalloc.h.c:#define TRY_LOCK(sl)          (!pthread_mutex_trylock(sl))
g_heap/dlmalloc.h.c:static MLOCK_T malloc_global_mutex = PTHREAD_MUTEX_INITIALIZER;
g_heap/dlmalloc.h.c:/* skipped internal declaration from pthread.h */
g_heap/dlmalloc.h.c:#ifndef PTHREAD_MUTEX_RECURSIVE
g_heap/dlmalloc.h.c:extern int pthread_mutexattr_setkind_np __P ((pthread_mutexattr_t *__attr,
g_heap/dlmalloc.h.c:#define PTHREAD_MUTEX_RECURSIVE PTHREAD_MUTEX_RECURSIVE_NP
g_heap/dlmalloc.h.c:#define pthread_mutexattr_settype(x,y) pthread_mutexattr_setkind_np(x,y)
g_heap/dlmalloc.h.c:static int pthread_init_lock (MLOCK_T *sl) {
g_heap/dlmalloc.h.c:  pthread_mutexattr_t attr;
g_heap/dlmalloc.h.c:  if (pthread_mutexattr_init(&attr)) return 1;
g_heap/dlmalloc.h.c:  if (pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE)) return 1;
g_heap/dlmalloc.h.c:  if (pthread_mutex_init(sl, &attr)) return 1;
g_heap/dlmalloc.h.c:  if (pthread_mutexattr_destroy(&attr)) return 1;
g_heap/dlmalloc.h.c:#define CURRENT_THREAD        GetCurrentThreadId()
hdf5_stats.cpp: * (alternatively, we could have an extra thread exclusively dedicated to writing stats).
init.cpp:        assert(numCores <= MAX_THREADS); //TODO: Is there any reason for this limit?
init.cpp:    uint32_t numSimThreads = config.get<uint32_t>("sim.contentionThreads", MAX((uint32_t)1, zinfo->numDomains/2)); //gives a bit of parallelism, TODO tune
init.cpp:    zinfo->contentionSim = new ContentionSim(zinfo->numDomains, numSimThreads);
init.cpp:    if (zinfo->ffReinstrument) warn("sim.ffReinstrument = true, switching fast-forwarding on a multi-threaded process may be unstable");
init.cpp:    zinfo->registerThreads = config.get<bool>("sim.registerThreads", false);
init.cpp:        if (parallelism < zinfo->numCores) info("Limiting concurrent threads to %d", parallelism);
init.cpp:        warn("sim.blockingSyscalls = True, will likely deadlock with multi-threaded apps!");
locks.h: * WARNING: Will not work with more than 64K threads
locks.h:            // Do linear backoff instead of a single mm_pause; this reduces ping-ponging, and allows more time for the other hyperthread
mtrand.h:// Not thread safe (unless auto-initialization is avoided and each thread has
null_core.cpp:void NullCore::LoadFunc(THREADID tid, ADDRINT addr) {}
null_core.cpp:void NullCore::StoreFunc(THREADID tid, ADDRINT addr) {}
null_core.cpp:void NullCore::PredLoadFunc(THREADID tid, ADDRINT addr, BOOL pred) {}
null_core.cpp:void NullCore::PredStoreFunc(THREADID tid, ADDRINT addr, BOOL pred) {}
null_core.cpp:void NullCore::BblFunc(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {
null_core.cpp:        //NOTE: TakeBarrier may take ownership of the core, and so it will be used by some other thread. If TakeBarrier context-switches us,
null_core.h://A core model with IPC=1 and no hooks into the memory hierarchy. Useful to isolate threads that need to be run for simulation purposes.
null_core.h:        static void LoadFunc(THREADID tid, ADDRINT addr);
null_core.h:        static void StoreFunc(THREADID tid, ADDRINT addr);
null_core.h:        static void BblFunc(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo);
null_core.h:        static void PredLoadFunc(THREADID tid, ADDRINT addr, BOOL pred);
null_core.h:        static void PredStoreFunc(THREADID tid, ADDRINT addr, BOOL pred);
null_core.h:        static void BranchFunc(THREADID, ADDRINT, BOOL, ADDRINT, ADDRINT) {}
ooo_core.cpp:void OOOCore::LoadFunc(THREADID tid, ADDRINT addr) {static_cast<OOOCore*>(cores[tid])->load(addr);}
ooo_core.cpp:void OOOCore::StoreFunc(THREADID tid, ADDRINT addr) {static_cast<OOOCore*>(cores[tid])->store(addr);}
ooo_core.cpp:void OOOCore::PredLoadFunc(THREADID tid, ADDRINT addr, BOOL pred) {
ooo_core.cpp:void OOOCore::PredStoreFunc(THREADID tid, ADDRINT addr, BOOL pred) {
ooo_core.cpp:void OOOCore::BblFunc(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {
ooo_core.cpp:        // NOTE: TakeBarrier may take ownership of the core, and so it will be used by some other thread. If TakeBarrier context-switches us,
ooo_core.cpp:void OOOCore::BranchFunc(THREADID tid, ADDRINT pc, BOOL taken, ADDRINT takenNpc, ADDRINT notTakenNpc) {
ooo_core.h:        static void LoadFunc(THREADID tid, ADDRINT addr);
ooo_core.h:        static void StoreFunc(THREADID tid, ADDRINT addr);
ooo_core.h:        static void PredLoadFunc(THREADID tid, ADDRINT addr, BOOL pred);
ooo_core.h:        static void PredStoreFunc(THREADID tid, ADDRINT addr, BOOL pred);
ooo_core.h:        static void BblFunc(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo);
ooo_core.h:        static void BranchFunc(THREADID tid, ADDRINT pc, BOOL taken, ADDRINT takenNpc, ADDRINT notTakenNpc);
ooo_core_recorder.cpp:        //but if the thread has not joined back by the end of phase, chances are this is a long leave
pin_cmd.cpp:    args.push_back("-tool_exit_timeout"); //don't wait much of internal threads
scheduler.cpp:        int res = syscall(SYS_nanosleep, &req, &rem); //we don't call glibc's nanosleep because errno is not thread-safe in pintools.
scheduler.cpp:/* Hacky way to figure out if a thread is sleeping on a certain futex.
scheduler.cpp:void Scheduler::watchdogThreadFunc() {
scheduler.cpp:    info("Started scheduler watchdog thread");
scheduler.cpp:            info("Terminating scheduler watchdog thread");
scheduler.cpp:        //if (lastPhase == curPhase && scheduledThreads == outQueue.size() && !sleepQueue.empty()) info("Mult %d curPhase %ld", multiplier, curPhase);
scheduler.cpp:                ThreadInfo* th = fl->th;
scheduler.cpp:                        // also do real leave for other threads blocked at the same pc ...
scheduler.cpp:        if (lastPhase == curPhase && scheduledThreads == outQueue.size() && !sleepQueue.empty()) {
scheduler.cpp:            //info("Watchdog Thread: Sleep dep detected...")
scheduler.cpp:            if (lastPhase == curPhase && scheduledThreads == outQueue.size() && !sleepQueue.empty()) {
scheduler.cpp:                ThreadInfo* sth = sleepQueue.front();
scheduler.cpp:                    info("Watchdog Thread: Driving time forward to avoid deadlock on sleep (%ld -> %ld ms)", curMs, endMs);
scheduler.cpp:                        //info("Sched: Threads waiting on advance, startPhase %ld curPhase %ld", lastPhase, curPhase);
scheduler.cpp:                        info("Termination condition met inside watchdog thread loop, exiting");
scheduler.cpp:        if (terminateWatchdogThread) {
scheduler.cpp:    info("Finished scheduler watchdog thread");
scheduler.cpp:void Scheduler::threadTrampoline(void* arg) {
scheduler.cpp:    sched->watchdogThreadFunc();
scheduler.cpp:void Scheduler::startWatchdogThread() {
scheduler.cpp:    PIN_SpawnInternalThread(threadTrampoline, this, 64*1024, nullptr);
scheduler.cpp:    ThreadInfo* th = contexts[cid].curThread;
scheduler.cpp:    ThreadInfo* th = gidMap[getGid(pid, tid)];
scheduler.cpp:    ThreadInfo* th = gidMap[getGid(pid, tid)];
scheduler.cpp:    ThreadInfo* th = gidMap[getGid(pid, tid)];
scheduler.cpp:void Scheduler::futexWakeJoin(ThreadInfo* th) {  // may release schedLock
scheduler.cpp:void Scheduler::futexWaitJoin(ThreadInfo* th) {
scheduler.cpp:void Scheduler::finishFakeLeave(ThreadInfo* th) {
scheduler.cpp:void Scheduler::waitUntilQueued(ThreadInfo* th) {
scheduler.h: * - The OUT state is unnecessary. It is done as a weak link between a thread that left and its context to preserve affinity, but
scheduler.h: * - wakeup() takes a needsJoin param that is computed per thread, but the barrier operates per core. This discrepancy manifests itself
scheduler.h: * - It should be clearer who transisions threads/contexts among states (the thread taking the context, the one giving the context?),
scheduler.h:        enum ThreadState {
scheduler.h:            STARTED, //transient state, thread will do a join immediately after
scheduler.h:            SLEEPING, //inside a patched sleep syscall; no cid assigned, in sleepQueue; it is our responsibility to wake this thread up when its deadline arrives
scheduler.h:        void (*atSyncFunc)(void); //executed by syncing thread while others are waiting. Good for non-thread-safe stuff
scheduler.h:        struct ThreadInfo : GlobAlloc, InListNode<ThreadInfo> {
scheduler.h:            ThreadState state;
scheduler.h:            volatile ThreadInfo* handoffThread; //if at the end of a sync() this is not nullptr, we need to transfer our current context to the thread pointed here.
scheduler.h:            ThreadInfo(uint32_t _gid, uint32_t _linuxPid, uint32_t _linuxTid, const g_vector<bool>& _mask) :
scheduler.h:                InListNode<ThreadInfo>(), gid(_gid), linuxPid(_linuxPid), linuxTid(_linuxTid), mask(_mask)
scheduler.h:                handoffThread = nullptr;
scheduler.h:            ThreadInfo* curThread; //only current if used, otherwise nullptr
scheduler.h:        g_unordered_map<uint32_t, ThreadInfo*> gidMap;
scheduler.h:        InList<ThreadInfo> runQueue;
scheduler.h:        InList<ThreadInfo> outQueue;
scheduler.h:        InList<ThreadInfo> sleepQueue; //contains all the sleeping threads, it is ORDERED by wakeup time
scheduler.h:        volatile bool terminateWatchdogThread;
scheduler.h:        Counter threadsCreated, threadsFinished;
scheduler.h:        uint32_t scheduledThreads;
scheduler.h:        Scheduler(void (*_atSyncFunc)(void), uint32_t _parallelThreads, uint32_t _numCores, uint32_t _schedQuantum) :
scheduler.h:            atSyncFunc(_atSyncFunc), bar(_parallelThreads, this), numCores(_numCores), schedQuantum(_schedQuantum), rnd(0x5C73D9134)
scheduler.h:                contexts[i].curThread = nullptr;
scheduler.h:            scheduledThreads = 0;
scheduler.h:            blockingSyscalls.resize(MAX_THREADS /* TODO: max # procs */);
scheduler.h:            terminateWatchdogThread = false;
scheduler.h:            startWatchdogThread();
scheduler.h:            threadsCreated.init("thCr", "Threads created"); schedStats->append(&threadsCreated);
scheduler.h:            threadsFinished.init("thFn", "Threads finished"); schedStats->append(&threadsFinished);
scheduler.h:            idlePhases.init("idlePhases", "Phases with no thread active"); schedStats->append(&idlePhases);
scheduler.h:            idlePeriods.init("idlePeriods", "Periods with no thread active"); schedStats->append(&idlePeriods);
scheduler.h:            // - SYS_getpid because after a fork (where zsim calls ThreadStart),
scheduler.h:            gidMap[gid] = new ThreadInfo(gid, syscall(SYS_getpid), syscall(SYS_gettid), mask);
scheduler.h:            threadsCreated.inc();
scheduler.h:            ThreadInfo* th = gidMap[gid];
scheduler.h:                warn("RUNNING thread %d (cid %d) called finish(), trying leave() first", tid, th->cid);
scheduler.h:            threadsFinished.inc();
scheduler.h:            ThreadInfo* th = gidMap[gid];
scheduler.h:                ContextInfo* ctx = schedThread(th);
scheduler.h:            ThreadInfo* th = contexts[cid].curThread;
scheduler.h:                    ThreadInfo* cur = sleepQueue.front();
scheduler.h:                ThreadInfo* inTh = schedContext(ctx);
scheduler.h:                ThreadInfo* inTh = schedContext(ctx);
scheduler.h:            ThreadInfo* th = contexts[cid].curThread;
scheduler.h:            if (th->handoffThread) {
scheduler.h:                ThreadInfo* dst = const_cast<ThreadInfo*>(th->handoffThread);  // de-volatilize
scheduler.h:                th->handoffThread = nullptr;
scheduler.h:                ctx = schedThread(th);
scheduler.h:            assert(scheduledThreads <= numCores);
scheduler.h:            occHist.inc(scheduledThreads);
scheduler.h:            //Wake up all sleeping threads where deadline is met
scheduler.h:                ThreadInfo* th = sleepQueue.front();
scheduler.h:            ThreadInfo* th = gidMap[gid];
scheduler.h:            ThreadInfo* th = gidMap[gid];
scheduler.h:            ThreadInfo* th = gidMap[gid];
scheduler.h:            //Move to BLOCKED; thread will join pretty much immediately
scheduler.h:                warn("Scheduler:notifySleepEnd: Benign race on SLEEPING->BLOCKED transition, thread is already blocked");
scheduler.h:        void printThreadState(uint32_t pid, uint32_t tid) {
scheduler.h:            ThreadInfo* th = gidMap[gid];
scheduler.h:            terminateWatchdogThread = true;
scheduler.h:        //Walks the gidMap and calls leave/finish on all threads of the process. Not quite race-free,
scheduler.h:        //if you call this and any other thread in the process is still alive, then there is a
scheduler.h:            g_unordered_map<uint32_t, ThreadInfo*>::iterator it;
scheduler.h:        //Calling doProcessCleanup on multithreaded processes leads to races,
scheduler.h:        //so we'll just have the watchdog thread to it once we're gone
scheduler.h:        uint32_t getScheduledPid(uint32_t cid) const { return (contexts[cid].state == USED)? getPid(contexts[cid].curThread->gid) : (uint32_t)-1; }
scheduler.h:        void schedule(ThreadInfo* th, ContextInfo* ctx) {
scheduler.h:            assert(ctx->curThread == nullptr);
scheduler.h:            ctx->curThread = th;
scheduler.h:            scheduledThreads++;
scheduler.h:        void deschedule(ThreadInfo* th, ContextInfo* ctx, ThreadState targetState) {
scheduler.h:            assert(ctx->curThread == th);
scheduler.h:            ctx->curThread = nullptr;
scheduler.h:            scheduledThreads--;
scheduler.h:        void waitForContext(ThreadInfo* th) {
scheduler.h:                int futex_res = syscall(SYS_futex, &th->futexWord, FUTEX_WAIT, 1 /*a racing thread waking us up will change value to 0, and we won't block*/, nullptr, nullptr, 0);
scheduler.h:        void wakeup(ThreadInfo* th, bool needsJoin) {
scheduler.h:                    ss << " " << std::setw(2) << contexts[c].curThread->gid;
scheduler.h:                    if (contexts[c].curThread->state == RUNNING) ss << "r";
scheduler.h:                    else if (contexts[c].curThread->state == OUT) ss << "o";
scheduler.h:                    else panic("Invalid state cid=%d, threadState=%d", c, contexts[c].curThread->state);
scheduler.h:         * - schedThread(): Here's a thread that just became available; return either a ContextInfo* where to schedule it, or nullptr if none are available
scheduler.h:         * - schedContext(): Here's a context that just became available; return either a ThreadInfo* to schedule on it, or nullptr if none are available
scheduler.h:         * - schedTick(): Current quantum is over, hand off contexts to other threads as you see fit
scheduler.h:         * for thread and context states. Those state machines are implemented and handled elsewhere, except where strictly necessary.
scheduler.h:        ContextInfo* schedThread(ThreadInfo* th) {
scheduler.h:            //Third, try to steal from the outQueue (block a thread, take its cid)
scheduler.h:                ThreadInfo* outTh = outQueue.front();
scheduler.h:            //info("schedThread done, gid %d, success %d", th->gid, ctx != nullptr);
scheduler.h:        ThreadInfo* schedContext(ContextInfo* ctx) {
scheduler.h:            ThreadInfo* th = nullptr;
scheduler.h:            ThreadInfo* blockedTh = runQueue.front();  // null if empty
scheduler.h:             * schedThread would have matched them out. So, no need to prioritize the freeList.
scheduler.h:            ThreadInfo* th = runQueue.front();
scheduler.h:                        ThreadInfo* victimTh = ctx->curThread;
scheduler.h:                        victimTh->handoffThread = th;
scheduler.h:                ThreadInfo* pth = th;
scheduler.h:            info("Time slice ended, context-switched %d threads, runQueue size %ld, available %ld", contextSwitches, runQueue.size(), avail.size());
scheduler.h:        //Watchdog thread functions
scheduler.h:        /* With sleeping threads, we have to drive time forward if no thread is scheduled and some threads are sleeping; otherwise, we can deadlock.
scheduler.h:         * This initially was the responsibility of the last leaving thread, but led to horribly long syscalls being simulated. For example, if you
scheduler.h:         * have 2 threads, 1 is sleeping and the other one goes on a syscall, it had to drive time fwd to wake the first thread up, on the off-chance
scheduler.h:         * Instead, we have an auxiliary thread check for this condition periodically, and if all threads are sleeping or blocked, we just drive time
scheduler.h:        void startWatchdogThread();
scheduler.h:        void watchdogThreadFunc();
scheduler.h:        static void threadTrampoline(void* arg);
scheduler.h:     * Threads leave() on a syscall enter and join() when they return, which desyncs them from the simulation to prevent deadlock through syscalls.
scheduler.h:     * - Threads should call syscallLeave() and syscallJoin(), passing their PC and a small syscall descriptor for a few syscalls of interest.
scheduler.h:     *   the watchdog detect potential deadlocks, and desyncing the threads. To avoid frequent desyncs, it blacklists syscalls 
scheduler.h:     * - When the scheduler wakes up a sleeping thread (e.g., in a timeout syscall), it ensures the phase does not slip by.
scheduler.h:     * - When the scheduler sees a FUTEX_WAKE, it ensures we wait for the woken-up thread(s).
scheduler.h:            ThreadInfo* const th;
scheduler.h:            FakeLeaveInfo(uint64_t _pc, ThreadInfo* _th, int _syscallNumber, uint64_t _arg0, uint64_t _arg1) :
scheduler.h:        void futexWakeJoin(ThreadInfo* th);  // may release and re-acquire schedLock 
scheduler.h:        void futexWaitJoin(ThreadInfo* th);
scheduler.h:        void finishFakeLeave(ThreadInfo* th);
scheduler.h:        /* Must be called with schedLock held. Waits until the given thread is
scheduler.h:         * ensure that the waking thread won't skip a phase. May cause deadlock
scheduler.h:        void waitUntilQueued(ThreadInfo* th);
SConscript:env["LIBS"] += ["pthread"]
simple_core.cpp:void SimpleCore::LoadFunc(THREADID tid, ADDRINT addr) {
simple_core.cpp:void SimpleCore::StoreFunc(THREADID tid, ADDRINT addr) {
simple_core.cpp:void SimpleCore::PredLoadFunc(THREADID tid, ADDRINT addr, BOOL pred) {
simple_core.cpp:void SimpleCore::PredStoreFunc(THREADID tid, ADDRINT addr, BOOL pred) {
simple_core.cpp:void SimpleCore::BblFunc(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {
simple_core.cpp:        //NOTE: TakeBarrier may take ownership of the core, and so it will be used by some other thread. If TakeBarrier context-switches us,
simple_core.h:        static void LoadFunc(THREADID tid, ADDRINT addr);
simple_core.h:        static void StoreFunc(THREADID tid, ADDRINT addr);
simple_core.h:        static void BblFunc(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo);
simple_core.h:        static void PredLoadFunc(THREADID tid, ADDRINT addr, BOOL pred);
simple_core.h:        static void PredStoreFunc(THREADID tid, ADDRINT addr, BOOL pred);
simple_core.h:        static void BranchFunc(THREADID, ADDRINT, BOOL, ADDRINT, ADDRINT) {}
sorttrace.cpp: * has seen at least one access from every thread, then dumps the sorted trace
stats.h: *   or for efficiency reasons (e.g. the per-thread phase cycles count is
stats.h:        /* An aggregate stat is regular if all its children are 1) aggregate and 2) of the same type (e.g. all the threads).
timing_core.cpp:void TimingCore::LoadAndRecordFunc(THREADID tid, ADDRINT addr) {
timing_core.cpp:void TimingCore::StoreAndRecordFunc(THREADID tid, ADDRINT addr) {
timing_core.cpp:void TimingCore::BblAndRecordFunc(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {
timing_core.cpp:void TimingCore::PredLoadAndRecordFunc(THREADID tid, ADDRINT addr, BOOL pred) {
timing_core.cpp:void TimingCore::PredStoreAndRecordFunc(THREADID tid, ADDRINT addr, BOOL pred) {
timing_core.h:        static void LoadAndRecordFunc(THREADID tid, ADDRINT addr);
timing_core.h:        static void StoreAndRecordFunc(THREADID tid, ADDRINT addr);
timing_core.h:        static void BblAndRecordFunc(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo);
timing_core.h:        static void PredLoadAndRecordFunc(THREADID tid, ADDRINT addr, BOOL pred);
timing_core.h:        static void PredStoreAndRecordFunc(THREADID tid, ADDRINT addr, BOOL pred);
timing_core.h:        static void BranchFunc(THREADID, ADDRINT, BOOL, ADDRINT, ADDRINT) {}
virt/common.h:    bool isNopThread;
virt/common.h:        warn("[%d] %s:%d Failed app<->tool copy (%ld/%ld bytes copied)", PIN_ThreadId(), file, line, copiedBytes, sizeof(T));
virt/virt.h:void VirtSyscallEnter(THREADID tid, CONTEXT *ctxt, SYSCALL_STANDARD std, const char* patchRoot, bool isNopThread);
virt/virt.h:PostPatchAction VirtSyscallExit(THREADID tid, CONTEXT *ctxt, SYSCALL_STANDARD std);
virt/timeout.cpp:static struct timespec fakeTimeouts[MAX_THREADS]; //for syscalls that use timespec to indicate a timeout
virt/timeout.cpp:static bool inFakeTimeoutMode[MAX_THREADS];
virt/timeout.cpp:    return args.isNopThread || zinfo->procArray[procIdx]->isInFastForward();
virt/timeout.cpp:         * (do_futex), WAKE and WAKE_OP return the number of threads woken up,
virt/timeout.cpp:         * of threads woken up + requeued. However, these variants
virt/timeout.cpp:         * (futex_requeue) first try to wake the specified threads, then
virt/timeout.cpp:         * requeue as many other threads as they can.
virt/timeout.cpp:         * of SYS_futex that wake up threads (WAKE, REQUEUE, CMP_REQUEUE, ...)
virt/time.cpp:    return args.isNopThread || zinfo->procArray[procIdx]->isInFastForward();
virt/time.cpp:        case CLOCK_THREAD_CPUTIME_ID:
virt/time.cpp:            warn("clock_gettime() called with CLOCK_THREAD_CPUTIME_ID, faking with CLOCK_PROCESS_CPUTIME_ID");
virt/time.cpp:        clock_gettime(CLOCK_THREAD_CPUTIME_ID, &ts);
virt/time.cpp:        trace(TimeVirt, "THREAD_CPUTIME_ID %ld sec, %ld nsec", ts.tv_sec, ts.tv_nsec);
virt/virt.cpp:PostPatchFn postPatchFunctions[MAX_THREADS];
virt/virt.cpp:void VirtSyscallEnter(THREADID tid, CONTEXT *ctxt, SYSCALL_STANDARD std, const char* patchRoot, bool isNopThread) {
virt/virt.cpp:        postPatchFunctions[tid] = prePatchFunctions[syscall]({tid, ctxt, std, patchRoot, isNopThread});
virt/virt.cpp:PostPatchAction VirtSyscallExit(THREADID tid, CONTEXT *ctxt, SYSCALL_STANDARD std) {
zsim.cpp:static uint32_t cids[MAX_THREADS];
zsim.cpp:Core* cores[MAX_THREADS];
zsim.cpp:    assert(tid < MAX_THREADS);
zsim.cpp:    assert(tid < MAX_THREADS);
zsim.cpp:    //assert(tid < MAX_THREADS); //these assertions are fine, but getCid is called everywhere, so they are expensive!
zsim.cpp:VOID SimThreadStart(THREADID tid);
zsim.cpp:VOID SimThreadFini(THREADID tid);
zsim.cpp:VOID HandleMagicOp(THREADID tid, ADDRINT op);
zsim.cpp:VOID FakeCPUIDPre(THREADID tid, REG eax, REG ecx);
zsim.cpp:VOID FakeCPUIDPost(THREADID tid, ADDRINT* eax, ADDRINT* ebx, ADDRINT* ecx, ADDRINT* edx); //REG* eax, REG* ebx, REG* ecx, REG* edx);
zsim.cpp:VOID FakeRDTSCPost(THREADID tid, REG* eax, REG* edx);
zsim.cpp:VOID FFThread(VOID* arg);
zsim.cpp:InstrFuncPtrs fPtrs[MAX_THREADS] ATTR_LINE_ALIGNED; //minimize false sharing
zsim.cpp:VOID PIN_FAST_ANALYSIS_CALL IndirectLoadSingle(THREADID tid, ADDRINT addr) {
zsim.cpp:VOID PIN_FAST_ANALYSIS_CALL IndirectStoreSingle(THREADID tid, ADDRINT addr) {
zsim.cpp:VOID PIN_FAST_ANALYSIS_CALL IndirectBasicBlock(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {
zsim.cpp:VOID PIN_FAST_ANALYSIS_CALL IndirectRecordBranch(THREADID tid, ADDRINT branchPc, BOOL taken, ADDRINT takenNpc, ADDRINT notTakenNpc) {
zsim.cpp:VOID PIN_FAST_ANALYSIS_CALL IndirectPredLoadSingle(THREADID tid, ADDRINT addr, BOOL pred) {
zsim.cpp:VOID PIN_FAST_ANALYSIS_CALL IndirectPredStoreSingle(THREADID tid, ADDRINT addr, BOOL pred) {
zsim.cpp:VOID JoinAndLoadSingle(THREADID tid, ADDRINT addr) {
zsim.cpp:VOID JoinAndStoreSingle(THREADID tid, ADDRINT addr) {
zsim.cpp:VOID JoinAndBasicBlock(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {
zsim.cpp:VOID JoinAndRecordBranch(THREADID tid, ADDRINT branchPc, BOOL taken, ADDRINT takenNpc, ADDRINT notTakenNpc) {
zsim.cpp:VOID JoinAndPredLoadSingle(THREADID tid, ADDRINT addr, BOOL pred) {
zsim.cpp:VOID JoinAndPredStoreSingle(THREADID tid, ADDRINT addr, BOOL pred) {
zsim.cpp:VOID NOPLoadStoreSingle(THREADID tid, ADDRINT addr) {}
zsim.cpp:VOID NOPBasicBlock(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {}
zsim.cpp:VOID NOPRecordBranch(THREADID tid, ADDRINT addr, BOOL taken, ADDRINT takenNpc, ADDRINT notTakenNpc) {}
zsim.cpp:VOID NOPPredLoadStoreSingle(THREADID tid, ADDRINT addr, BOOL pred) {}
zsim.cpp:VOID FFBasicBlock(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {
zsim.cpp:        SimThreadStart(tid);
zsim.cpp: * REQUIREMENTS: Single-threaded during FF (non-FF can be MT)
zsim.cpp:VOID FFIBasicBlock(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {
zsim.cpp:        SimThreadStart(tid);
zsim.cpp:VOID FFIEntryBasicBlock(THREADID tid, ADDRINT bblAddr, BblInfo* bblInfo) {
zsim.cpp:    // Done before tick() to avoid deadlock in most cases when entering synced ffwd (can we still deadlock with sleeping threads?)
zsim.cpp:        info("Thread %d entering fast-forward", tid);
zsim.cpp:        SimThreadFini(tid);
zsim.cpp:static void PrintIp(THREADID tid, ADDRINT ip) {
zsim.cpp:    //INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR)PrintIp, IARG_THREAD_ID, IARG_REG_VALUE, REG_INST_PTR, IARG_END);
zsim.cpp:                INS_InsertCall(ins, IPOINT_BEFORE, LoadFuncPtr, IARG_FAST_ANALYSIS_CALL, IARG_THREAD_ID, IARG_MEMORYREAD_EA, IARG_END);
zsim.cpp:                INS_InsertCall(ins, IPOINT_BEFORE, PredLoadFuncPtr, IARG_FAST_ANALYSIS_CALL, IARG_THREAD_ID, IARG_MEMORYREAD_EA, IARG_EXECUTING, IARG_END);
zsim.cpp:                INS_InsertCall(ins, IPOINT_BEFORE, LoadFuncPtr, IARG_FAST_ANALYSIS_CALL, IARG_THREAD_ID, IARG_MEMORYREAD2_EA, IARG_END);
zsim.cpp:                INS_InsertCall(ins, IPOINT_BEFORE, PredLoadFuncPtr, IARG_FAST_ANALYSIS_CALL, IARG_THREAD_ID, IARG_MEMORYREAD2_EA, IARG_EXECUTING, IARG_END);
zsim.cpp:                INS_InsertCall(ins, IPOINT_BEFORE,  StoreFuncPtr, IARG_FAST_ANALYSIS_CALL, IARG_THREAD_ID, IARG_MEMORYWRITE_EA, IARG_END);
zsim.cpp:                INS_InsertCall(ins, IPOINT_BEFORE,  PredStoreFuncPtr, IARG_FAST_ANALYSIS_CALL, IARG_THREAD_ID, IARG_MEMORYWRITE_EA, IARG_EXECUTING, IARG_END);
zsim.cpp:            INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR) IndirectRecordBranch, IARG_FAST_ANALYSIS_CALL, IARG_THREAD_ID,
zsim.cpp:        INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR) HandleMagicOp, IARG_THREAD_ID, IARG_REG_VALUE, REG_ECX, IARG_END);
zsim.cpp:       INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR) FakeCPUIDPre, IARG_THREAD_ID, IARG_REG_VALUE, REG_EAX, IARG_REG_VALUE, REG_ECX, IARG_END);
zsim.cpp:       INS_InsertCall(ins, IPOINT_AFTER, (AFUNPTR) FakeCPUIDPost, IARG_THREAD_ID, IARG_REG_REFERENCE, REG_EAX,
zsim.cpp:        INS_InsertCall(ins, IPOINT_AFTER, (AFUNPTR) FakeRDTSCPost, IARG_THREAD_ID, IARG_REG_REFERENCE, REG_EAX, IARG_REG_REFERENCE, REG_EDX, IARG_END);
zsim.cpp:                 IARG_THREAD_ID, IARG_ADDRINT, BBL_Address(bbl), IARG_PTR, bblInfo, IARG_END);
zsim.cpp:// Per-thread VDSO data
zsim.cpp:VdsoPatchData vdsoPatchData[MAX_THREADS];
zsim.cpp:VOID VdsoEntryPoint(THREADID tid, uint32_t func, ADDRINT arg0, ADDRINT arg1) {
zsim.cpp:VOID VdsoCallPoint(THREADID tid) {
zsim.cpp:VOID VdsoRetPoint(THREADID tid, REG* raxPtr) {
zsim.cpp:            INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR) VdsoEntryPoint, IARG_THREAD_ID, IARG_UINT32, (uint32_t)func, IARG_REG_VALUE, REG_RDI, IARG_REG_VALUE, REG_RSI, IARG_END);
zsim.cpp:            INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR) VdsoCallPoint, IARG_THREAD_ID, IARG_END);
zsim.cpp:            INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR) VdsoRetPoint, IARG_THREAD_ID, IARG_REG_REFERENCE, REG_RAX /* return val */, IARG_END);
zsim.cpp:bool activeThreads[MAX_THREADS];  // set in ThreadStart, reset in ThreadFini, we need this for exec() (see FollowChild)
zsim.cpp:bool inSyscall[MAX_THREADS];  // set in SyscallEnter, reset in SyscallExit, regardless of state. We MAY need this for ContextChange
zsim.cpp:uint32_t CountActiveThreads() {
zsim.cpp:    // Finish all threads in this process w.r.t. the global scheduler
zsim.cpp:    for (uint32_t i = 0; i < MAX_THREADS; i++) {
zsim.cpp:        if (activeThreads[i]) activeCount++;
zsim.cpp:void SimThreadStart(THREADID tid) {
zsim.cpp:    info("Thread %d starting", tid);
zsim.cpp:    if (tid > MAX_THREADS) panic("tid > MAX_THREADS");
zsim.cpp:    activeThreads[tid] = true;
zsim.cpp:        //int result = pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
zsim.cpp:    //Initialize this thread's process-local data
zsim.cpp:VOID ThreadStart(THREADID tid, CONTEXT *ctxt, INT32 flags, VOID *v) {
zsim.cpp:    /* This should only fire for the first thread; I know this is a callback,
zsim.cpp:     * It's here and not in main() because that way the auxiliary threads can
zsim.cpp:        info("FF thread %d starting", tid);
zsim.cpp:    } else if (zinfo->registerThreads) {
zsim.cpp:        info("Shadow thread %d starting", tid);
zsim.cpp:        //Start normal thread
zsim.cpp:        SimThreadStart(tid);
zsim.cpp:VOID SimThreadFini(THREADID tid) {
zsim.cpp:    activeThreads[tid] = false;
zsim.cpp:VOID ThreadFini(THREADID tid, const CONTEXT *ctxt, INT32 flags, VOID *v) {
zsim.cpp:    //NOTE: Thread has no valid cid here!
zsim.cpp:        info("Shadow/NOP thread %d finished", tid);
zsim.cpp:        SimThreadFini(tid);
zsim.cpp:        info("Thread %d finished", tid);
zsim.cpp://Need to remove ourselves from running threads in case the syscall is blocking
zsim.cpp:VOID SyscallEnter(THREADID tid, CONTEXT *ctxt, SYSCALL_STANDARD std, VOID *v) {
zsim.cpp:    bool isNopThread = fPtrs[tid].type == FPTR_NOP;
zsim.cpp:    VirtSyscallEnter(tid, ctxt, std, procTreeNode->getPatchRoot(), isNopThread);
zsim.cpp:    if (isNopThread) return;
zsim.cpp:VOID SyscallExit(THREADID tid, CONTEXT *ctxt, SYSCALL_STANDARD std, VOID *v) {
zsim.cpp:        assert(activeThreads[tid]);
zsim.cpp:        info("Thread %d entering fast-forward (from syscall exit)", tid);
zsim.cpp:        SimThreadFini(tid);
zsim.cpp:VOID ContextChange(THREADID tid, CONTEXT_CHANGE_REASON reason, const CONTEXT* from, CONTEXT* to, INT32 info, VOID* v) {
zsim.cpp:    //Finish all threads in this process w.r.t. the global scheduler
zsim.cpp:    uint32_t activeCount = CountActiveThreads();
zsim.cpp:    if (activeCount > 1) warn("exec() of a multithreaded process! (%d live threads)", activeCount);
zsim.cpp:    if (procIdx == 0) panic("process0 cannot exec(), it spawns globally needed internal threads (scheduler and contention); run a dummy process0 instead!");
zsim.cpp:VOID BeforeFork(THREADID tid, const CONTEXT* ctxt, VOID * arg) {
zsim.cpp:    info("Thread %d forking, child procIdx=%d", tid, forkedChildNode->getProcIdx());
zsim.cpp:VOID AfterForkInParent(THREADID tid, const CONTEXT* ctxt, VOID * arg) {
zsim.cpp:VOID AfterForkInChild(THREADID tid, const CONTEXT* ctxt, VOID * arg) {
zsim.cpp:    info("Forked child (tid %d/%d), PID %d, parent PID %d", tid, PIN_ThreadId(), PIN_GetPid(), getppid());
zsim.cpp:    //Initialize process-local per-thread state, even if ThreadStart does so later
zsim.cpp:    for (uint32_t i = 0; i < MAX_THREADS; i++) {
zsim.cpp:        activeThreads[i] = false;
zsim.cpp:    //We need to launch another copy of the FF control thread
zsim.cpp:    PIN_SpawnInternalThread(FFThread, nullptr, 64*1024, nullptr);
zsim.cpp:    ThreadStart(tid, nullptr, 0, nullptr);
zsim.cpp:        while (true) { //sleep until thread that won exits for us
zsim.cpp:        //Done to preserve the scheduler and contention simulation internal threads
zsim.cpp:    //Uncomment when debugging termination races, which can be rare because they are triggered by threads of a dying process
zsim.cpp:#define ZSIM_MAGIC_OP_REGISTER_THREAD   (1027)
zsim.cpp:VOID HandleMagicOp(THREADID tid, ADDRINT op) {
zsim.cpp:                //TODO: Test whether this is thread-safe
zsim.cpp:                //TODO: Test whether this is thread-safe
zsim.cpp:                        info("Thread %d entering fast-forward (immediate)", tid);
zsim.cpp:                        SimThreadFini(tid);
zsim.cpp:        case ZSIM_MAGIC_OP_REGISTER_THREAD:
zsim.cpp:            if (!zinfo->registerThreads) {
zsim.cpp:                info("Thread %d: Treating REGISTER_THREAD magic op as NOP", tid);
zsim.cpp:                    SimThreadStart(tid);
zsim.cpp:                    warn("Thread %d: Treating REGISTER_THREAD magic op as NOP, thread already registered", tid);
zsim.cpp:            panic("Thread %d issued unknown magic op %ld!", tid, op);
zsim.cpp:static uint32_t cpuidEax[MAX_THREADS];
zsim.cpp:static uint32_t cpuidEcx[MAX_THREADS];
zsim.cpp:VOID FakeCPUIDPre(THREADID tid, REG eax, REG ecx) {
zsim.cpp:VOID FakeCPUIDPost(THREADID tid, ADDRINT* eax, ADDRINT* ebx, ADDRINT* ecx, ADDRINT* edx) {
zsim.cpp:VOID FakeRDTSCPost(THREADID tid, REG* eax, REG* edx) {
zsim.cpp:    if (fPtrs[tid].type == FPTR_NOP) return; //avoid virtualizing NOP threads.
zsim.cpp:// Helper class, enabled the FFControl thread to sync with the phase end code
zsim.cpp:        // Unblocks thread that called wait(), blocks until signal() called
zsim.cpp:        // Unblocks thread waiting in callback()
zsim.cpp:VOID FFThread(VOID* arg) {
zsim.cpp:    info("FF control Thread TID %ld", syscall(SYS_gettid));
zsim.cpp:                info("Terminating FF control thread");
zsim.cpp:            //info("FF control thread wakeup");
zsim.cpp:            //At this point the thread thet triggered the end of phase is blocked inside of EndOfPhaseActions
zsim.cpp:                info("FF control thread called on end of phase, but someone else (program?) already entered ffwd");
zsim.cpp:            syncEv->signal(); //unblock thread in EndOfPhaseActions
zsim.cpp:static EXCEPT_HANDLING_RESULT InternalExceptionHandler(THREADID tid, EXCEPTION_INFO *pExceptInfo, PHYSICAL_CONTEXT *pPhysCtxt, VOID *) {
zsim.cpp:    //Initialize process-local per-thread state, even if ThreadStart does so later
zsim.cpp:    for (uint32_t i = 0; i < MAX_THREADS; i++) {
zsim.cpp:    PIN_AddThreadStartFunction(ThreadStart, 0);
zsim.cpp:    PIN_AddThreadFiniFunction(ThreadFini, 0);
zsim.cpp:    //OK, screw it. Launch this on a separate thread, and forget about signals... the caller will set a shared memory var. PIN is hopeless with signal instrumentation on multithreaded processes!
zsim.cpp:    PIN_SpawnInternalThread(FFThread, nullptr, 64*1024, nullptr);
zsim.h:    uint64_t maxMinInstrs; //terminate when all threads have reached this many instructions
zsim.h:    // If true, threads start as shadow and have no effect on simulation until they call the register magic op
zsim.h:    bool registerThreads;
zsim.h:    //If true, all the regular aggregate stats are summed before dumped, e.g. getting one thread record with instrs&cycles for all the threads
zsim.h:extern Core* cores[MAX_THREADS]; //tid->core array
zsim.h:void SimEnd(); //only call point out of zsim.cpp should be watchdog threads
zsim_harness.cpp://At most as many processes as threads, plus one extra process per child if we launch a debugger
zsim_harness.cpp:#define MAX_CHILDREN (2*MAX_THREADS)
zsim_harness.cpp:volatile uint32_t debuggerChildIdx = MAX_THREADS;
zsim_harness.cpp:        if (idx < MAX_THREADS) {
